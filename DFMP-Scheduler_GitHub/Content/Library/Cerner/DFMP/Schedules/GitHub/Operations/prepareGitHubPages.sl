########################################################################################################################
#!!
#! @description: This Python operation sets the full reference URL path in HTML href values, generates article hash and builds the string containing smaxService , external_id, title, article_hash and article_body delimited by "♪".
#!
#! @input githubPagesBaseUrl: GitHub Pages base url
#! @input gitHubActualPageURL: GitHub Pages Main repo page url
#! @input gitHubActualPageResponse: Response data of GET of actual repo Page
#! @input github_page_tagsStr: GitHub Pages Tags list for validation - this is a list with the full URL containing the tag in it.
#! @input smaxService: SMAX Service Definition ID
#!
#! @output service_article: String containing smaxService , external_id, title, article_hash and article_body delimited by "♪"
#! @output new_external_id: New External Article ID
#!!#
########################################################################################################################
namespace: Cerner.DFMP.Schedules.GitHub.Operations
operation:
  name: prepareGitHubPages
  inputs:
    - githubPagesBaseUrl
    - gitHubActualPageURL
    - gitHubActualPageResponse
    - github_page_tagsStr
    - smaxService
  python_action:
    use_jython: false
    script: "# This Operation is to prepare the Github Article Page for smax\n# Developed by Sandeep Yesudas\n# Operation : prepareGitHubPages\n# Input:\n#       -   githubPagesBaseUrl\n#       -   gitHubActualPageURL\n#       -   gitHubActualPageResponse\n#       -   github_page_tagsStr\n#       -   smaxService\n#\n# Outputs:\n#       -   result\n#       -   message\n#       -   errorType\n#       -   errorMessage\n#       -   errorSeverity\n#       -   errorProvder\n#       -   errorType\n#       -   service_article\n#       -   new_external_id\n\n\n# Modified on 06 Dec 2022 by Rakesh Sharma for Removing the Extra Chars, Tags, Categories and adding Page Links for the Graph Images\n\n##############################################################\n\nimport sys, os\nimport subprocess\n\n# function do download external modules to python \"on-the-fly\"\ndef install(param):\n    message = \"\"\n    result = \"\"\n    try:\n\n        pathname = os.path.dirname(sys.argv[0])\n        message = os.path.abspath(pathname)\n        message = subprocess.call([sys.executable, \"-m\", \"pip\", \"list\"])\n        message = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", param], capture_output=True)\n        result = \"True\"\n    except Exception as e:\n        message = e\n        result = \"False\"\n    return {\"result\": result, \"message\": message}\n\n# requirement external modules\ninstall(\"bs4\")\n\n\ndef execute(githubPagesBaseUrl, gitHubActualPageURL, gitHubActualPageResponse, github_page_tagsStr, smaxService):\n\n    message = \"\"\n    result = \"False\"\n    errorType = \"\"\n    errorSeverity = \"\"\n    errorProvider = \"\"\n    errorMessage = \"\"\n    errorLogs = \"\"\n    service_article = ''\n    new_external_id = ''\n\n    try:\n        import requests\n        import json\n        \n        href_correct = 'href=\"'+githubPagesBaseUrl+\"/\"\n        github_page_tags = github_page_tagsStr.split(\"^\")\n        response = gitHubActualPageResponse\n        gitapi_url = gitHubActualPageURL\n        tresponse = response\n        \n        #Get the resp\n        git_repo = gitHubActualPageURL.replace('//','').split('/')[1]\n        \n        # fix the broken Images if any\n        response = response.replace('<img src=\"../../','<img src=\"' + githubPagesBaseUrl + '/' + git_repo + '/' )\n\n        \n        # Get Main Article Body\n        from bs4 import BeautifulSoup\n        soup = BeautifulSoup(response, 'html.parser')\n        response = soup.find(\"main\")\n        if not response:\n            msg = 'The Required Main Class not found the in returned Githb Page'\n            raise Exception(msg)\n        \n        # Remove the Nav Class for removing unnecesary links of Documentatoin etc.\n        if '<nav' in str(response):\n            response = str(response).split('<nav',1)[0] + str(response).split('</nav>',1)[1]\n            \n        ## Check if any Graph Image in Article then replace it with the Page links\n        graphlink = \"<br><div><p><a href=\\\"\" + gitapi_url + \"\\\"> Click here to access the Graph Image. </a>&nbsp;</p><div><br/>\"\n        \n        if '<div class=\"mermaid\">' in response:\n            while '<div class=\"mermaid\">' in response:\n                response = str(response).split('<div class=\"mermaid\">',1)[0] + graphlink + str(response).split('<div class=\"mermaid\">',1)[1].split('</div>',1)[1]\n\n        if any(tagsPath in tresponse for tagsPath in github_page_tags):\n            result = \"True\"\n            #article_content = response\n            title = tresponse.split(\"<title>\")[1].split(\"</title>\")[0]\n            headingAppend = \"<div><p>Original page can be found at: <a href=\\\"\" + gitapi_url + \"\\\">\" + gitapi_url + \"</a>&nbsp;</p><div><br/>\"\n            article_body = str(response).replace('href=\"/', href_correct)\n            article_body = headingAppend+article_body\n            article_body = pageCleanup(article_body)[\"outHTML\"]\n\n            # generate hash of Article URL as external ID\n            external_id = createHash(gitapi_url)[\"hash_object\"]\n\n            new_external_id = external_id\n            article_hash_response = createHash(article_body)\n            article_hash = article_hash_response[\"hash_object\"]\n            service_article = smaxService + \"♪\" + external_id + \"♪\" + title + \"♪\" + article_hash + \"♪\" + article_body\n        else:\n            result= \"True\"\n            message = \"GitHub Pages do not match any Tags\"\n            service_article = ''\n            new_external_id = ''\n\n    except Exception as e:\n        message = str(e)\n        result = \"False\"\n        errorMessage = message\n        errorType = 'e30000'\n        if not errorProvider:\n            errorProvider = 'OO'\n        errorSeverity = \"ERROR\"\n        errorLogs = \"ProviderUrl,||ErrorProvider,OO||ProviderUrlBody,||ErrorMessage,\" + str(message) + \"|||\"\n    return {\"result\": result, \"message\": message, \"errorType\": errorType, \"errorSeverity\": errorSeverity,\"errorProvider\": errorProvider,\"errorMessage\":errorMessage,\"errorLogs\":errorLogs, \"service_article\": service_article, \"new_external_id\": new_external_id}\n\n# Content hash function\ndef createHash(inputString):\n    message = \"\"\n    result = \"\"\n    hash_object = \"\"\n    try:\n        import hashlib\n        # Assumes the default UTF-8\n        hash_object = hashlib.md5(inputString.encode()).hexdigest()\n        result = \"True\"\n    except Exception as e:\n        message = e\n        result = \"False\"\n    return {\"result\": result, \"message\": message, \"hash_object\": hash_object}\n\n\n# HTML convertion for Confluence HTML content\ndef getHTML(inHTML, link, github_user, github_password):\n    message = \"\"\n    result = \"\"\n    outHTML = \"\"\n\n    try:\n\n        from lxml import html, etree\n        import gh_md_to_html\n\n        # print(\"     Processing: \" + link)\n        link_template = \"<div><p>Original page can be found at: <a href=\\\"\" + link + \"\\\">\" + link + \"</a>&nbsp;</p><div><br/>\"\n        xslt_text = \"<xsl:stylesheet version=\\\"1.0\\\"\\r\\n xmlns:xsl=\\\"http://www.w3.org/1999/XSL/Transform\\\"\\r\\n xmlns:xhtml=\\\"http://www.w3.org/1999/xhtml\\\">\\r\\n <xsl:output omit-xml-declaration=\\\"yes\\\" indent=\\\"yes\\\"/>\\r\\n <xsl:strip-space elements=\\\"*\\\"/>\\r\\n\\r\\n <xsl:template match=\\\"p|div|br|a|h1|h2|h3|li|ul|ol|u|strong|table|td|tr|img|span|@*\\\">\\r\\n     <xsl:copy>\\r\\n       <xsl:apply-templates select=\\\"node()|@*\\\"/>\\r\\n     </xsl:copy>\\r\\n </xsl:template>\\r\\n\\r\\n <xsl:template match=\\\"code\\\"/>\\r\\n</xsl:stylesheet>\"\n\n        #inHTML = gh_md_to_html.markdown_to_html_via_github_api(inHTML)\n        #inHTML = markdown_github_pages(inHTML, github_user, github_password)[\"responseVal\"]\n        xslt_doc = etree.fromstring(xslt_text)\n        inHTML = inHTML.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n        transform = etree.XSLT(xslt_doc)\n        ehtml = html.fromstring(inHTML)\n        docs = etree.tostring(ehtml)\n        result = transform(ehtml)\n\n        docs = etree.tostring(result)\n        #docs = str(result)\n\n        outHTMLraw = link_template + str(bytes.decode(docs).replace(\"\\&quot;\", \"\").replace(\"\\\\n\", \"\").replace(\"&#194;&#160;\", \"&nbsp;\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\")) + \" \"\n        #outHTML = link_template + str(docs).replace(\"\\&quot;\", \"\").replace(\"\\\\n\", \"\").replace(\"&#194;&#160;\", \"&nbsp;\").replace(\n         #       \"&lt;\", \"<\").replace(\"&gt;\", \">\") + \" \"\n        outHTML = pageCleanup(outHTMLraw)[\"outHTML\"]\n         # print(\"     Processing +DONE+\\n\")\n        result = \"True\"\n    except Exception as e:\n        message = e\n        print(message)\n        result = \"False\"\n    return {\"result\": result, \"message\": message, \"outHTML\": outHTML}\n    # return inHTML\n\n#Below function is used for cleaning Search bar and empty bullet points from the GitHub Pages before updating/ deleting SMAX Article\ndef pageCleanup(HTMLraw):\n    message = \"\"\n    result = \"\"\n    outHTML = \"\"\n\n    try:\n        \n         #---- This section removes the search bar content from html page------\n        startString = '<p>type=\"search\"class='\n        endString = 'data-offline-search-max-results=\"10\"</div>'\n        endString2 = 'data-offline-search-max-results=\"10\"<ul>'\n        if HTMLraw.find(startString) > 0:\n            start_index = HTMLraw.index(startString)\n            if HTMLraw.find(endString) > 0:\n                end_index = HTMLraw.index(endString)+41\n                HTMLraw2 = HTMLraw[0: start_index:] + HTMLraw[end_index + 1::]\n            elif HTMLraw.find(endString2) > 0:\n                end_index = HTMLraw.index(endString2)+35\n                HTMLraw2 = HTMLraw[0: start_index:] + HTMLraw[end_index + 1::]\n            else:\n                HTMLraw2 =  HTMLraw\n        else:\n            HTMLraw2 =  HTMLraw\n         #------ clean search bar ends here------------------------------------\n         #---- This section removes the empty bullet points from html page------\n        startStringBullet = '<footer class'\n        #endStringBullet = '</div></div></div></div>'\n        if HTMLraw2.find(startStringBullet) > 0:\n            upto_index = HTMLraw2.index(startStringBullet)\n            outHTML = HTMLraw2[0: upto_index]\n        else:\n            outHTML =  HTMLraw2\n         #------ clean empty bullet points ends here------------------------------------\n        #------ fix for truncated breadcrumb text -------------------------------------\n        breadcrumbStart = '<ol class=\"breadcrumb\">'\n        breadcrumbEnd = '</ol>'\n        breadcrumbStartPos = outHTML.find(breadcrumbStart)\n        if breadcrumbStartPos > -1:\n            breadcrumbEndPos = outHTML.find(breadcrumbEnd, breadcrumbStartPos)\n            #outHTML = outHTML[0:breadcrumbStartPos]+'<div>'+outHTML[breadcrumbStartPos+23:breadcrumbEndPos]+'</div>'+outHTML[breadcrumbEndPos+5:]\n            tempHTML = outHTML[breadcrumbStartPos+23:breadcrumbEndPos]\n            listTagreplaceComplete = False\n            breadCumbStartTag = False\n            listStartStr = '<li'\n            listEndStr = '<a href'\n            while listTagreplaceComplete == False:\n                listStartStrPos = tempHTML.find(listStartStr)\n                listEndStrPos = tempHTML.find(listEndStr)\n                if listStartStrPos > -1 and listEndStrPos > listStartStrPos:\n                    tempHTML = tempHTML[listEndStrPos:]\n                    breadCumbStartTag = True\n                else:\n                    listTagreplaceComplete = True\n            if breadCumbStartTag == True:\n                tempHTML = tempHTML.replace('</li>',\"\").replace(\"\\n\", \"\").strip()\n            outHTML = outHTML[0:breadcrumbStartPos]+'<div>'+tempHTML+'</div>'+outHTML[breadcrumbEndPos+5:]\n         #------ fix for truncated breadcrumb text ends here----------------------------\n    except Exception as e:\n        message = e\n        print(message)\n        result = \"False\"\n    return {\"result\": result, \"message\": message, \"outHTML\": outHTML}\n    \ndef markdown_github_pages(markdowninput, github_user, github_password):\n\timport requests\n\tbasicAuthCredentials = (github_user, github_password)    \n\theaders = {\"Content-Type\": \"text/plain\", \"charset\": \"utf-8\"}\n\ts = requests.Session()\n\tres = s.get('https://github.cerner.com')\n\tcookies = dict(res.cookies)\n\tresponseVal = str(requests.post(\"https://github.cerner.com/api/markdown\", headers=headers, cookies=cookies, data=markdowninput.encode(\"utf-8\"), auth=basicAuthCredentials).content, encoding=\"utf-8\")\n\treturn {\"responseVal\": responseVal}"
  outputs:
    - service_article
    - new_external_id
    - result
    - message
    - errorType
    - errorSeverity
    - errorProvider
    - errorMessage
    - errorLogs
  results:
    - SUCCESS: "${result=='True'}"
    - FAILURE
